#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Include your name and ID here.
#
# Describe how and why you modified the baseline code.
#
##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:

##################################################################
# You can modify this portion
	# Loop header
	# the %rax is initially 0, so delete xor %rax, %rax
	jmp Test1		# goto Test1:

# 6x1unrolling
Loop1:	mrmovq (%rdi), %r8	# read val from src...
	mrmovq 8(%rdi), %r9
	rmmovq %r8, (%rsi)	# ...and store it to dst
	rmmovq %r9, 8(%rsi)
	andq %r8, %r8		# val <= 0?
	jle Loop2		# if so, goto Loop1:
	iaddq $1, %rax		# count++
Loop2:	andq %r9, %r9
	jle Loop3
	iaddq $1, %rax
Loop3:	mrmovq 16(%rdi), %r8
	mrmovq 24(%rdi), %r9
	rmmovq %r8, 16(%rsi)
	rmmovq %r9, 24(%rsi)
	andq %r8, %r8
	jle Loop4
	iaddq $1, %rax
Loop4:	andq %r9, %r9
	jle Loop5
	iaddq $1, %rax
Loop5:	mrmovq 32(%rdi), %r8
	mrmovq 40(%rdi), %r9
	rmmovq %r8, 32(%rsi)
	rmmovq %r9, 40(%rsi)
	andq %r8, %r8
	jle Loop6
	iaddq $1, %rax
Loop6:	iaddq $48, %rdi
	iaddq $48, %rsi
	andq %r9, %r9
	jle Test1
	iaddq $1, %rax
# deal with the left, 3x1unrolling
Test1:	iaddq $-6, %rdx
	jge Loop1
    iaddq $6, %rdx
    jmp Test2	
Left1:	mrmovq (%rdi), %r8
	mrmovq 8(%rdi), %r9
    rmmovq %r8, (%rsi)
	rmmovq %r9, 8(%rsi)
    andq %r8, %r8
    jle Left2
    iaddq $1, %rax
Left2:andq %r9, %r9
    jle Left3
    iaddq $1, %rax
Left3:	mrmovq 16(%rdi), %r8
	iaddq $24, %rdi
    rmmovq %r8, 16(%rsi)
	iaddq $24, %rsi
    andq %r8, %r8
    jle Test2
    iaddq $1, %rax
# deal with the remain of the left
Test2:  iaddq $-3, %rdx
    jge Left1
	iaddq $2, %rdx
	jl Done				# len - 1 < 0, done
	je Remain1			# len - 1 = 0, 1 remain
	mrmovq (%rdi), %r8	# else 2 remain
	mrmovq 8(%rdi), %r9
	rmmovq %r8, (%rsi)
	rmmovq %r9, 8(%rsi)
	andq %r8, %r8
	jle Remain2
	iaddq $1, %rax
Remain2:andq %r9, %r9
	jle Done
	iaddq $1, %rax
	jmp Done
Remain1:mrmovq (%rdi), %r8
	andq %r8, %r8
	rmmovq %r8, (%rsi)
	jle Done 
	iaddq $1, %rax
##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
	ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad 1
	.quad -2
	.quad -3
	.quad 4
	.quad 5
	.quad 6
	.quad -7
	.quad -8
	.quad 9
	.quad 10
	.quad 11
	.quad 12
	.quad 13
	.quad 14
	.quad 15
	.quad -16
	.quad -17
	.quad 18
	.quad 19
	.quad -20
	.quad 21
	.quad -22
	.quad -23
	.quad -24
	.quad 25
	.quad -26
	.quad 27
	.quad 28
	.quad 29
	.quad 30
	.quad 31
	.quad -32
	.quad -33
	.quad -34
	.quad -35
	.quad 36
	.quad -37
	.quad -38
	.quad 39
	.quad -40
	.quad 41
	.quad 42
	.quad -43
	.quad 44
	.quad 45
	.quad -46
	.quad -47
	.quad 48
	.quad -49
	.quad -50
	.quad -51
	.quad 52
	.quad 53
	.quad -54
	.quad 55
	.quad 56
	.quad -57
	.quad -58
	.quad -59
	.quad -60
	.quad -61
	.quad -62
	.quad -63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
